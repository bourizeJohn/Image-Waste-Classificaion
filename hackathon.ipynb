{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1743380340724
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (2.2.3)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (4.66.5)\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: matplotlib in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (3.9.2)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: tqdm, threadpoolctl, joblib, scikit-learn, matplotlib\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.9.2\n",
            "    Uninstalling matplotlib-3.9.2:\n",
            "      Successfully uninstalled matplotlib-3.9.2\n",
            "Successfully installed joblib-1.4.2 matplotlib-3.10.1 scikit-learn-1.6.1 threadpoolctl-3.6.0 tqdm-4.67.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "## **IMPORTING LIBRARIES**\n",
        "\n",
        "%pip install --upgrade pandas scikit-learn tqdm matplotlib\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "#import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "importing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1743380349504
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download complete. File saved to Users/pemphokatsala/DATA/waste_classification.zip\n",
            "File size: 223781993 bytes (213.42 MB)\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "# Create directory for the dataset\n",
        "os.makedirs('Users/pemphokatsala/DATA', exist_ok=True)\n",
        "\n",
        "# Download the dataset\n",
        "dataset_url = \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/n3gtgm9jxj-2.zip\"\n",
        "zip_path = \"Users/pemphokatsala/DATA/waste_classification.zip\"\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "urllib.request.urlretrieve(dataset_url, zip_path)\n",
        "print(f\"Download complete. File saved to {zip_path}\")\n",
        "\n",
        "# Check the file size\n",
        "file_size = os.path.getsize(zip_path)\n",
        "print(f\"File size: {file_size} bytes ({file_size/1024/1024:.2f} MB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "extracting dataset from zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting zip file...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete!\n",
            "\n",
            "Extracted contents:\n",
            "extracted/\n",
            "    Waste Classification Dataset/\n",
            "        waste_dataset/\n",
            "            .amlignore\n",
            "            .amlignore.amltmp\n",
            "            waste_dataset_augmentation.ipynb\n",
            "            waste_dataset_CNN.ipynb\n",
            "            waste_dataset_README.txt\n",
            "            organic/\n",
            "                organic_000001_photo.jpg\n",
            "                organic_000002_photo.jpg\n",
            "                organic_000003_photo.jpg\n",
            "                organic_000004_photo.jpg\n",
            "                organic_000005_photo.jpg\n",
            "                ... and 13875 more files\n",
            "            recyclable/\n",
            "                recyclable_000001_photo.jpg\n",
            "                recyclable_000002_photo.jpg\n",
            "                recyclable_000003_photo.jpg\n",
            "                recyclable_000004_photo.jpg\n",
            "                recyclable_000005_photo.jpg\n",
            "                ... and 10820 more files\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "extract_path = \"/home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/extracted\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    print(\"Extracting zip file...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete!\")\n",
        "\n",
        "    # List the contents of the extracted directory\n",
        "    print(\"\\nExtracted contents:\")\n",
        "    for root, dirs, files in os.walk(extract_path, topdown=True):\n",
        "        level = root.replace(extract_path, '').count(os.sep)\n",
        "        indent = ' ' * 4 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        sub_indent = ' ' * 4 * (level + 1)\n",
        "        for f in files[:5]:  # Show only first 5 files in each directory\n",
        "            print(f\"{sub_indent}{f}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{sub_indent}... and {len(files)-5} more files\")\n",
        "\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"Error: The file is not a valid zip file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        " Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking dataset paths...\n",
            "Path exists: /home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/extracted/Waste Classification Dataset/waste_dataset/organic\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contains 13880 images\n",
            "Path exists: /home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/extracted/Waste Classification Dataset/waste_dataset/recyclable\n",
            "Contains 10825 images\n",
            "\n",
            "=== STEP 1: Cleaning the dataset ===\n",
            "Processing 13880 images from /home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/extracted/Waste Classification Dataset/waste_dataset/organic\n",
            "  - Removed 0 duplicate files\n",
            "  - Identified 0 corrupted images\n",
            "  - Missing dimensions: 0 width, 0 height\n",
            "  - Imputed missing dimensions with median values\n",
            "  - Removed 21 outlier images\n",
            "  - Removed 0 corrupted images\n",
            "  - Final count: 13859 clean images\n",
            "\n",
            "Processing 10825 images from /home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/extracted/Waste Classification Dataset/waste_dataset/recyclable\n",
            "  - Removed 0 duplicate files\n",
            "  - Identified 0 corrupted images\n",
            "  - Missing dimensions: 0 width, 0 height\n",
            "  - Imputed missing dimensions with median values\n",
            "  - Removed 44 outlier images\n",
            "  - Removed 0 corrupted images\n",
            "  - Final count: 10781 clean images\n",
            "\n",
            "Total clean images across all directories: 24640\n",
            "\n",
            "=== STEP 2: Adding category labels ===\n",
            "Category distribution:\n",
            "organic       13859\n",
            "recyclable    10781\n",
            "Name: category, dtype: int64\n",
            "\n",
            "=== STEP 3: Saving standardized images ===\n",
            "Created output directory: /home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/cleaned_waste_dataset\n",
            "Created category directory: /home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/cleaned_waste_dataset/organic\n",
            "Saved 100 images...\n",
            "Saved 200 images...\n",
            "Saved 300 images...\n",
            "Saved 400 images...\n",
            "Saved 500 images...\n",
            "Saved 600 images...\n",
            "Saved 700 images...\n",
            "Saved 800 images...\n",
            "Saved 900 images...\n",
            "Saved 1000 images...\n",
            "Saved 1100 images...\n",
            "Saved 1200 images...\n",
            "Saved 1300 images...\n",
            "Saved 1400 images...\n",
            "Saved 1500 images...\n",
            "Saved 1600 images...\n",
            "Saved 1700 images...\n",
            "Saved 1800 images...\n",
            "Saved 1900 images...\n",
            "Saved 2000 images...\n",
            "Saved 2100 images...\n",
            "Saved 2200 images...\n",
            "Saved 2300 images...\n",
            "Saved 2400 images...\n",
            "Saved 2500 images...\n",
            "Saved 2600 images...\n",
            "Saved 2700 images...\n",
            "Saved 2800 images...\n",
            "Saved 2900 images...\n",
            "Saved 3000 images...\n",
            "Saved 3100 images...\n",
            "Saved 3200 images...\n",
            "Saved 3300 images...\n",
            "Saved 3400 images...\n",
            "Saved 3500 images...\n",
            "Saved 3600 images...\n",
            "Saved 3700 images...\n",
            "Saved 3800 images...\n",
            "Saved 3900 images...\n",
            "Saved 4000 images...\n",
            "Saved 4100 images...\n",
            "Saved 4200 images...\n",
            "Saved 4300 images...\n",
            "Saved 4400 images...\n",
            "Saved 4500 images...\n",
            "Saved 4600 images...\n",
            "Saved 4700 images...\n",
            "Saved 4800 images...\n",
            "Saved 4900 images...\n",
            "Saved 5000 images...\n",
            "Saved 5100 images...\n",
            "Saved 5200 images...\n",
            "Saved 5300 images...\n",
            "Saved 5400 images...\n",
            "Saved 5500 images...\n",
            "Saved 5600 images...\n",
            "Saved 5700 images...\n",
            "Saved 5800 images...\n",
            "Saved 5900 images...\n",
            "Saved 6000 images...\n",
            "Saved 6100 images...\n",
            "Saved 6200 images...\n",
            "Saved 6300 images...\n",
            "Saved 6400 images...\n",
            "Saved 6500 images...\n",
            "Saved 6600 images...\n",
            "Saved 6700 images...\n",
            "Saved 6800 images...\n",
            "Saved 6900 images...\n",
            "Saved 7000 images...\n",
            "Saved 7100 images...\n",
            "Saved 7200 images...\n",
            "Saved 7300 images...\n",
            "Saved 7400 images...\n",
            "Saved 7500 images...\n",
            "Saved 7600 images...\n",
            "Saved 7700 images...\n",
            "Saved 7800 images...\n",
            "Saved 7900 images...\n",
            "Saved 8000 images...\n",
            "Saved 8100 images...\n",
            "Saved 8200 images...\n",
            "Saved 8400 images...\n",
            "Saved 8500 images...\n",
            "Saved 8600 images...\n",
            "Saved 8700 images...\n",
            "Saved 8800 images...\n",
            "Saved 8900 images...\n",
            "Saved 9000 images...\n",
            "Saved 9100 images...\n",
            "Saved 9200 images...\n",
            "Saved 9300 images...\n",
            "Saved 9400 images...\n",
            "Saved 9500 images...\n",
            "Saved 9600 images...\n",
            "Saved 9700 images...\n",
            "Saved 9800 images...\n",
            "Saved 9900 images...\n",
            "Saved 10000 images...\n",
            "Saved 10100 images...\n",
            "Saved 10200 images...\n",
            "Saved 10300 images...\n",
            "Saved 10400 images...\n",
            "Saved 10500 images...\n",
            "Saved 10600 images...\n",
            "Saved 10700 images...\n",
            "Saved 10800 images...\n",
            "Saved 10900 images...\n",
            "Saved 11000 images...\n",
            "Saved 11100 images...\n",
            "Saved 11200 images...\n",
            "Saved 11300 images...\n",
            "Saved 11400 images...\n",
            "Saved 11500 images...\n",
            "Saved 11600 images...\n",
            "Saved 11700 images...\n",
            "Saved 11800 images...\n",
            "Saved 11900 images...\n",
            "Saved 12000 images...\n",
            "Saved 12100 images...\n",
            "Saved 12200 images...\n",
            "Saved 12300 images...\n",
            "Saved 12400 images...\n",
            "Saved 12500 images...\n",
            "Saved 12600 images...\n",
            "Saved 12700 images...\n",
            "Saved 12800 images...\n",
            "Saved 12900 images...\n",
            "Saved 13000 images...\n",
            "Saved 13100 images...\n",
            "Saved 13200 images...\n",
            "Saved 13300 images...\n",
            "Saved 13400 images...\n",
            "Saved 13500 images...\n",
            "Saved 13600 images...\n",
            "Saved 13700 images...\n",
            "Saved 13800 images...\n",
            "Created category directory: /home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/cleaned_waste_dataset/recyclable\n",
            "Saved 13900 images...\n",
            "Saved 14000 images...\n",
            "Saved 14100 images...\n",
            "Saved 14200 images...\n",
            "Saved 14300 images...\n",
            "Saved 14400 images...\n",
            "Saved 14500 images...\n",
            "Saved 14600 images...\n",
            "Saved 14700 images...\n",
            "Saved 14800 images...\n",
            "Saved 14900 images...\n",
            "Saved 15000 images...\n",
            "Saved 15100 images...\n",
            "Saved 15200 images...\n",
            "Saved 15300 images...\n",
            "Saved 15400 images...\n",
            "Saved 15500 images...\n",
            "Saved 15600 images...\n",
            "Saved 15700 images...\n",
            "Saved 15800 images...\n",
            "Saved 15900 images...\n",
            "Saved 16000 images...\n",
            "Saved 16100 images...\n",
            "Saved 16200 images...\n",
            "Saved 16300 images...\n",
            "Saved 16400 images...\n",
            "Saved 16500 images...\n",
            "Saved 16600 images...\n",
            "Saved 16700 images...\n",
            "Saved 16800 images...\n",
            "Saved 16900 images...\n",
            "Saved 17000 images...\n",
            "Saved 17100 images...\n",
            "Saved 17200 images...\n",
            "Saved 17300 images...\n",
            "Saved 17400 images...\n",
            "Saved 17500 images...\n",
            "Saved 17600 images...\n",
            "Saved 17700 images...\n",
            "Saved 17800 images...\n",
            "Saved 17900 images...\n",
            "Saved 18000 images...\n",
            "Saved 18100 images...\n",
            "Saved 18200 images...\n",
            "Saved 18300 images...\n",
            "Saved 18400 images...\n",
            "Saved 18500 images...\n",
            "Saved 18600 images...\n",
            "Saved 18700 images...\n",
            "Saved 18800 images...\n",
            "Saved 18900 images...\n",
            "Saved 19000 images...\n",
            "Saved 19100 images...\n",
            "Saved 19200 images...\n",
            "Saved 19300 images...\n",
            "Saved 19400 images...\n",
            "Saved 19500 images...\n",
            "Saved 19600 images...\n",
            "Saved 19700 images...\n",
            "Saved 19800 images...\n",
            "Saved 19900 images...\n",
            "Saved 20000 images...\n",
            "Saved 20100 images...\n",
            "Saved 20200 images...\n",
            "Saved 20300 images...\n",
            "Saved 20400 images...\n",
            "Saved 20500 images...\n",
            "Saved 20600 images...\n",
            "Saved 20700 images...\n",
            "Saved 20800 images...\n",
            "Saved 20900 images...\n",
            "Saved 21000 images...\n",
            "Saved 21100 images...\n",
            "Saved 21200 images...\n",
            "Saved 21300 images...\n",
            "Saved 21400 images...\n",
            "Saved 21500 images...\n",
            "Saved 21600 images...\n",
            "Saved 21700 images...\n",
            "Saved 21800 images...\n",
            "Saved 21900 images...\n",
            "Saved 22000 images...\n",
            "Saved 22100 images...\n",
            "Saved 22200 images...\n",
            "Saved 22300 images...\n",
            "Saved 22400 images...\n",
            "Saved 22500 images...\n",
            "Saved 22600 images...\n",
            "Saved 22700 images...\n",
            "Saved 22800 images...\n",
            "Saved 22900 images...\n",
            "Saved 23000 images...\n",
            "Saved 23100 images...\n",
            "Saved 23200 images...\n",
            "Saved 23300 images...\n",
            "Saved 23400 images...\n",
            "Saved 23500 images...\n",
            "Saved 23600 images...\n",
            "Saved 23700 images...\n",
            "Saved 23800 images...\n",
            "Saved 23900 images...\n",
            "Saved 24000 images...\n",
            "Saved 24100 images...\n",
            "Saved 24200 images...\n",
            "Saved 24300 images...\n",
            "Saved 24400 images...\n",
            "Saved 24500 images...\n",
            "Saved 24600 images...\n",
            "Successfully saved 24640 cleaned images\n",
            "Category 'organic': 13859 images\n",
            "Category 'recyclable': 10781 images\n",
            "\n",
            "=== Process Complete ===\n",
            "Cleaned data saved to: /home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/cleaned_waste_dataset\n",
            "The cleaned dataset is now ready for model training\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def clean_data(dataset_paths):\n",
        "    \"\"\"Handles duplicates, missing data, outliers, and inconsistent data in image datasets.\"\"\"\n",
        "\n",
        "    all_dfs = []  # List to store DataFrames from each path\n",
        "\n",
        "    for dataset_path in dataset_paths:\n",
        "        # Check if path exists\n",
        "        if not os.path.exists(dataset_path):\n",
        "            print(f\"Warning: Path {dataset_path} does not exist. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Get all image files\n",
        "        try:\n",
        "            image_files = [f for f in os.listdir(dataset_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n",
        "        except Exception as e:\n",
        "            print(f\"Error accessing directory {dataset_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Create dataframe\n",
        "        data = {'filename': image_files, 'filepath': [os.path.join(dataset_path, f) for f in image_files]}\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No images found in {dataset_path}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {len(df)} images from {dataset_path}\")\n",
        "\n",
        "        # 1. Handle Duplicates (based on filename)\n",
        "        before_drop = len(df)\n",
        "        df.drop_duplicates(subset='filename', keep='first', inplace=True)\n",
        "        print(f\"  - Removed {before_drop - len(df)} duplicate files\")\n",
        "\n",
        "        # 2. Handle Corrupted Images\n",
        "        df['corrupted'] = False  # Initialize column properly\n",
        "        corrupted_count = 0\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            try:\n",
        "                img = Image.open(row['filepath'])\n",
        "                img.verify()  # Verify image\n",
        "                # Also try to load it to catch other potential issues\n",
        "                img = Image.open(row['filepath'])\n",
        "                img.load()\n",
        "            except Exception as e:\n",
        "                df.at[index, 'corrupted'] = True\n",
        "                corrupted_count += 1\n",
        "\n",
        "        print(f\"  - Identified {corrupted_count} corrupted images\")\n",
        "\n",
        "        # 3. Handle Missing Data (image dimensions)\n",
        "        dimensions = []\n",
        "        for filepath in df['filepath']:\n",
        "            try:\n",
        "                if os.path.exists(filepath):\n",
        "                    img = Image.open(filepath)\n",
        "                    width, height = img.size\n",
        "                    dimensions.append((width, height))\n",
        "                else:\n",
        "                    dimensions.append((None, None))\n",
        "            except Exception:\n",
        "                dimensions.append((None, None))\n",
        "\n",
        "        df[['width', 'height']] = pd.DataFrame(dimensions, index=df.index)\n",
        "\n",
        "        # Count missing values before imputation\n",
        "        missing_width = df['width'].isna().sum()\n",
        "        missing_height = df['height'].isna().sum()\n",
        "        print(f\"  - Missing dimensions: {missing_width} width, {missing_height} height\")\n",
        "\n",
        "        # Impute missing dimensions if there are any non-missing values\n",
        "        if not df[['width', 'height']].isna().all().all():\n",
        "            imputer = SimpleImputer(strategy='median')\n",
        "            df[['width', 'height']] = imputer.fit_transform(df[['width', 'height']])\n",
        "            print(f\"  - Imputed missing dimensions with median values\")\n",
        "\n",
        "        # 4. Handle Outliers (image dimensions)\n",
        "        before_outlier = len(df)\n",
        "\n",
        "        # Only process if we have enough data for meaningful quartiles\n",
        "        if len(df) > 10:\n",
        "            # Function to detect and mark outliers\n",
        "            def mark_outliers(df, column):\n",
        "                q1 = df[column].quantile(0.25)\n",
        "                q3 = df[column].quantile(0.75)\n",
        "                iqr = q3 - q1\n",
        "                lower_bound = q1 - (1.5 * iqr)\n",
        "                upper_bound = q3 + (1.5 * iqr)\n",
        "                return ~((df[column] >= lower_bound) & (df[column] <= upper_bound))\n",
        "\n",
        "            # Mark outliers for both dimensions\n",
        "            df['width_outlier'] = mark_outliers(df, 'width')\n",
        "            df['height_outlier'] = mark_outliers(df, 'height')\n",
        "\n",
        "            # Remove rows where both width and height are outliers\n",
        "            outliers = df['width_outlier'] & df['height_outlier']\n",
        "            df = df[~outliers]\n",
        "\n",
        "            # Clean up the temporary columns\n",
        "            df = df.drop(['width_outlier', 'height_outlier'], axis=1)\n",
        "\n",
        "            print(f\"  - Removed {before_outlier - len(df)} outlier images\")\n",
        "\n",
        "        # 5. Handle Inconsistent Data (filename case)\n",
        "        df['filename'] = df['filename'].str.lower()\n",
        "\n",
        "        # Remove corrupted images from final dataset\n",
        "        before_corrupt_removal = len(df)\n",
        "        df = df[df['corrupted'] == False]\n",
        "        print(f\"  - Removed {before_corrupt_removal - len(df)} corrupted images\")\n",
        "\n",
        "        # Drop the corrupted column\n",
        "        df = df.drop('corrupted', axis=1)\n",
        "\n",
        "        print(f\"  - Final count: {len(df)} clean images\\n\")\n",
        "        all_dfs.append(df)  # Append the cleaned DataFrame to the list\n",
        "\n",
        "    # Concatenate all DataFrames if we have any\n",
        "    if all_dfs:\n",
        "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "        print(f\"Total clean images across all directories: {len(combined_df)}\")\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"No valid images found in any of the provided paths.\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame if no valid data\n",
        "\n",
        "# Add a category label based on the directory\n",
        "def add_category_labels(df):\n",
        "    \"\"\"Add category labels based on the filepath\"\"\"\n",
        "    df['category'] = df['filepath'].apply(lambda x: os.path.basename(os.path.dirname(x)))\n",
        "    return df\n",
        "\n",
        "def save_cleaned_data(cleaned_df, output_base_dir, target_size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Save cleaned data to a new directory structure, preserving categories and standardizing images.\n",
        "\n",
        "    Args:\n",
        "        cleaned_df: DataFrame with 'filepath' and 'filename' columns\n",
        "        output_base_dir: Base directory to save cleaned data\n",
        "        target_size: Tuple of (width, height) to resize images to\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping original categories to new save paths\n",
        "    \"\"\"\n",
        "    # Create the base output directory\n",
        "    os.makedirs(output_base_dir, exist_ok=True)\n",
        "    print(f\"Created output directory: {output_base_dir}\")\n",
        "\n",
        "    # Create a dictionary to track categories and their save paths\n",
        "    category_paths = {}\n",
        "    saved_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    # For each file in the cleaned DataFrame\n",
        "    for idx, row in cleaned_df.iterrows():\n",
        "        try:\n",
        "            # Extract category from original filepath\n",
        "            category = os.path.basename(os.path.dirname(row['filepath']))\n",
        "\n",
        "            # Create category directory if it doesn't exist in our tracking dict\n",
        "            if category not in category_paths:\n",
        "                category_dir = os.path.join(output_base_dir, category)\n",
        "                os.makedirs(category_dir, exist_ok=True)\n",
        "                category_paths[category] = category_dir\n",
        "                print(f\"Created category directory: {category_dir}\")\n",
        "\n",
        "            # Load the original image\n",
        "            img = Image.open(row['filepath'])\n",
        "\n",
        "            # Standardize to desired format (RGB, specific size)\n",
        "            img = img.convert('RGB')\n",
        "            img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "            # Define save path\n",
        "            save_path = os.path.join(category_paths[category], row['filename'])\n",
        "\n",
        "            # Save the image\n",
        "            img.save(save_path)\n",
        "            saved_count += 1\n",
        "\n",
        "            # Print progress update for every 100 images\n",
        "            if saved_count % 100 == 0:\n",
        "                print(f\"Saved {saved_count} images...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving {row['filepath']}: {e}\")\n",
        "            error_count += 1\n",
        "\n",
        "    print(f\"Successfully saved {saved_count} cleaned images\")\n",
        "    if error_count > 0:\n",
        "        print(f\"Encountered errors with {error_count} images\")\n",
        "\n",
        "    # Report counts per category\n",
        "    for category, path in category_paths.items():\n",
        "        count = len([f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))])\n",
        "        print(f\"Category '{category}': {count} images\")\n",
        "\n",
        "    return category_paths\n",
        "\n",
        "# Main function to execute the entire workflow\n",
        "def main():\n",
        "    # Define the paths to your dataset directories\n",
        "    filepath1 = r'/home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/extracted/Waste Classification Dataset/waste_dataset/organic'\n",
        "    filepath2 = r'/home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/extracted/Waste Classification Dataset/waste_dataset/recyclable'\n",
        "\n",
        "    # Define output directory for cleaned data\n",
        "    cleaned_output_dir = \"/home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/cleaned_waste_dataset\"\n",
        "\n",
        "    # Check if directories exist\n",
        "    print(\"Checking dataset paths...\")\n",
        "    for path in [filepath1, filepath2]:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"Path exists: {path}\")\n",
        "            print(f\"Contains {len([f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))])} images\")\n",
        "        else:\n",
        "            print(f\"Path does not exist: {path}\")\n",
        "\n",
        "    # Only proceed with paths that exist\n",
        "    valid_paths = [path for path in [filepath1, filepath2] if os.path.exists(path)]\n",
        "\n",
        "    if valid_paths:\n",
        "        print(\"\\n=== STEP 1: Cleaning the dataset ===\")\n",
        "        cleaned_df = clean_data(valid_paths)\n",
        "\n",
        "        if not cleaned_df.empty:\n",
        "            print(\"\\n=== STEP 2: Adding category labels ===\")\n",
        "            labeled_df = add_category_labels(cleaned_df)\n",
        "            print(\"Category distribution:\")\n",
        "            print(labeled_df['category'].value_counts())\n",
        "\n",
        "            print(\"\\n=== STEP 3: Saving standardized images ===\")\n",
        "            # Save cleaned and standardized images\n",
        "            category_paths = save_cleaned_data(labeled_df, cleaned_output_dir, target_size=(256, 256))\n",
        "\n",
        "            print(\"\\n=== Process Complete ===\")\n",
        "            print(f\"Cleaned data saved to: {cleaned_output_dir}\")\n",
        "            print(\"The cleaned dataset is now ready for model training\")\n",
        "        else:\n",
        "            print(\"No valid images found after cleaning. Please check your dataset.\")\n",
        "    else:\n",
        "        print(\"No valid paths to process. Please check your directory structure.\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "image processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tensorflow/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tensorflow/python/eager/context.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cancellation\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m executor\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_conversion_registry\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Type, Sequence, Optional\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/ml_dtypes/__init__.py:40\u001b[0m\n\u001b[1;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m ]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Type\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_finfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m finfo\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_iinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iinfo\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/ml_dtypes/_finfo.py:153\u001b[0m\n\u001b[1;32m    149\u001b[0m     smallest_subnormal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m.\u001b[39mfromhex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0x1p-127\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmallest_subnormal \u001b[38;5;241m=\u001b[39m float8_e8m0fnu(smallest_subnormal)\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mfinfo\u001b[39;00m(np\u001b[38;5;241m.\u001b[39mfinfo):  \u001b[38;5;66;03m# pylint: disable=invalid-name,missing-class-docstring\u001b[39;00m\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[1;32m    156\u001b[0m   \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    157\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_bfloat16_finfo\u001b[39m():\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/ml_dtypes/_finfo.py:699\u001b[0m, in \u001b[0;36mfinfo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    684\u001b[0m _finfo_type_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    685\u001b[0m     _bfloat16_dtype: _bfloat16_finfo,\n\u001b[1;32m    686\u001b[0m     _float4_e2m1fn_dtype: _float4_e2m1fn_finfo,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m     _float8_e8m0fnu_dtype: _float8_e8m0fnu_finfo,\n\u001b[1;32m    697\u001b[0m }\n\u001b[1;32m    698\u001b[0m _finfo_name_map \u001b[38;5;241m=\u001b[39m {t\u001b[38;5;241m.\u001b[39mname: t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m _finfo_type_map}\n\u001b[0;32m--> 699\u001b[0m _finfo_cache \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    700\u001b[0m     t: init_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__func__\u001b[39m() \u001b[38;5;28;01mfor\u001b[39;00m t, init_fn \u001b[38;5;129;01min\u001b[39;00m _finfo_type_map\u001b[38;5;241m.\u001b[39mitems()  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    701\u001b[0m }\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dtype):\n\u001b[1;32m    704\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mstr\u001b[39m):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/ml_dtypes/_finfo.py:700\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    684\u001b[0m _finfo_type_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    685\u001b[0m     _bfloat16_dtype: _bfloat16_finfo,\n\u001b[1;32m    686\u001b[0m     _float4_e2m1fn_dtype: _float4_e2m1fn_finfo,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m     _float8_e8m0fnu_dtype: _float8_e8m0fnu_finfo,\n\u001b[1;32m    697\u001b[0m }\n\u001b[1;32m    698\u001b[0m _finfo_name_map \u001b[38;5;241m=\u001b[39m {t\u001b[38;5;241m.\u001b[39mname: t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m _finfo_type_map}\n\u001b[1;32m    699\u001b[0m _finfo_cache \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 700\u001b[0m     t: \u001b[43minit_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__func__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, init_fn \u001b[38;5;129;01min\u001b[39;00m _finfo_type_map\u001b[38;5;241m.\u001b[39mitems()  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    701\u001b[0m }\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dtype):\n\u001b[1;32m    704\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mstr\u001b[39m):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/ml_dtypes/_finfo.py:668\u001b[0m, in \u001b[0;36mfinfo._float8_e8m0fnu_finfo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    667\u001b[0m obj\u001b[38;5;241m.\u001b[39m_machar \u001b[38;5;241m=\u001b[39m _Float8E8m0fnuMachArLike()\n\u001b[0;32m--> 668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtiny\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    669\u001b[0m   obj\u001b[38;5;241m.\u001b[39mtiny \u001b[38;5;241m=\u001b[39m float8_e8m0fnu(tiny)\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmallest_normal\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/numpy/core/getlimits.py:578\u001b[0m, in \u001b[0;36mtiny\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/numpy/core/getlimits.py:557\u001b[0m, in \u001b[0;36msmallest_normal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
          ]
        }
      ],
      "source": [
        "import tensorflow.keras.preprocessing.image "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-image\n",
            "  Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting numpy>=1.24 (from scikit-image)\n",
            "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-image) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-image) (3.3)\n",
            "Requirement already satisfied: pillow>=10.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-image) (10.4.0)\n",
            "Collecting imageio!=2.35.0,>=2.33 (from scikit-image)\n",
            "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
            "  Downloading tifffile-2025.3.30-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: packaging>=21 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-image) (24.1)\n",
            "Collecting lazy-loader>=0.4 (from scikit-image)\n",
            "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
            "Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
            "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tifffile-2025.3.30-py3-none-any.whl (226 kB)\n",
            "Installing collected packages: numpy, lazy-loader, tifffile, imageio, scikit-image\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "azureml-dataset-runtime 1.57.0 requires numpy!=1.19.3,<1.24; sys_platform == \"linux\", but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed imageio-2.37.0 lazy-loader-0.4 numpy-2.2.4 scikit-image-0.25.2 tifffile-2025.3.30\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset composition:\n",
            "  Organic: 13859 images\n",
            "  Recyclable: 10781 images\n",
            "\n",
            "Feature extraction progress:\n",
            "\n",
            "Processing completed!\n",
            "Final dataset size: 24640 samples\n",
            "Feature vector length: 30752\n",
            "Saved to: /home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/processed_waste_dataset\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from skimage.feature import hog\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "def extract_features(image_path):\n",
        "    \"\"\"Extract HOG features from 256x256 normalized images\"\"\"\n",
        "    try:\n",
        "        # Load and standardize image size\n",
        "        img = load_img(image_path, target_size=(256, 256))\n",
        "\n",
        "        # Normalize pixel values to [0,1]\n",
        "        img_array = img_to_array(img) / 255.0\n",
        "\n",
        "        # Convert to grayscale for HOG\n",
        "        img_gray = rgb2gray(img_array)\n",
        "\n",
        "        # Extract HOG features\n",
        "        return hog(img_gray,\n",
        "                 orientations=8,\n",
        "                 pixels_per_cell=(8, 8),\n",
        "                 cells_per_block=(2, 2),\n",
        "                 transform_sqrt=True)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error processing {image_path}: {str(e)}\")\n",
        "\n",
        "def build_dataset(base_dir):\n",
        "    \"\"\"Create structured dataset from directory\"\"\"\n",
        "    image_data = []\n",
        "    class_counts = {'organic': 0, 'recyclable': 0}\n",
        "\n",
        "    for class_name in ['organic', 'recyclable']:\n",
        "        class_dir = os.path.join(base_dir, class_name)\n",
        "        if not os.path.exists(class_dir):\n",
        "            raise FileNotFoundError(f\"Missing directory: {class_dir}\")\n",
        "\n",
        "        for fname in os.listdir(class_dir):\n",
        "            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                image_data.append({\n",
        "                    'path': os.path.join(class_dir, fname),\n",
        "                    'label': class_name\n",
        "                })\n",
        "                class_counts[class_name] += 1\n",
        "\n",
        "    print(\"Dataset composition:\")\n",
        "    print(f\"  Organic: {class_counts['organic']} images\")\n",
        "    print(f\"  Recyclable: {class_counts['recyclable']} images\")\n",
        "    return pd.DataFrame(image_data)\n",
        "\n",
        "# Main processing workflow\n",
        "base_dir = \"/home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/cleaned_waste_dataset\"\n",
        "df = build_dataset(base_dir)\n",
        "\n",
        "# Feature extraction\n",
        "features = []\n",
        "labels = []\n",
        "valid_paths = []\n",
        "\n",
        "print(\"\\nFeature extraction progress:\")\n",
        "for idx, row in df.iterrows():\n",
        "    try:\n",
        "        features.append(extract_features(row['path']))\n",
        "        labels.append(row['label'])\n",
        "        valid_paths.append(row['path'])\n",
        "    except Exception as e:\n",
        "        print(f\"Skipped {row['path']}: {str(e)}\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "features_array = np.array(features)\n",
        "labels_array = LabelEncoder().fit_transform(labels)\n",
        "\n",
        "# Save processed data\n",
        "output_dir = \"/home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/processed_waste_dataset\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "np.savez(os.path.join(output_dir, 'processed_data.npz'),\n",
        "         features=features_array,\n",
        "         labels=labels_array)\n",
        "\n",
        "pd.DataFrame({'path': valid_paths, 'label': labels})\\\n",
        "  .to_csv(os.path.join(output_dir, 'metadata.csv'), index=False)\n",
        "\n",
        "print(\"\\nProcessing completed!\")\n",
        "print(f\"Final dataset size: {features_array.shape[0]} samples\")\n",
        "print(f\"Feature vector length: {features_array.shape[1]}\")\n",
        "print(f\"Saved to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "data scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standardized data stats:\n",
            "Mean: -0.0000\n",
            "Std: 1.0000\n",
            "[[-0.92890495 -0.6085843  -0.642102   ... -0.65024    -0.6757899\n",
            "  -0.65263134]\n",
            " [ 2.0051558  -0.6085843   1.1146678  ... -0.65024    -0.6757899\n",
            "  -0.65263134]\n",
            " [ 1.7420782   3.8632255   3.4329965  ... -0.65024    -0.6757899\n",
            "  -0.65263134]\n",
            " [-0.92890495 -0.6085843  -0.642102   ... -0.65024    -0.6757899\n",
            "  -0.65263134]\n",
            " [-0.92890495 -0.6085843  -0.642102   ... -0.65024    -0.6757899\n",
            "  -0.65263134]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "\n",
        "# 1. Load features in memory-mapped mode\n",
        "features_memmap = np.load('/home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/processed_waste_dataset/processed_data.npz', mmap_mode='r')\n",
        "features = features_memmap['features']\n",
        "\n",
        "# 2. Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 3. Chunked fitting\n",
        "chunk_size = 1000  # Adjust based on available RAM\n",
        "for i in range(0, len(features), chunk_size):\n",
        "    chunk = features[i:i+chunk_size]\n",
        "    scaler.partial_fit(chunk)  # Incremental mean/variance calculation\n",
        "\n",
        "# 4. Chunked transformation with disk backing\n",
        "output_path = '/home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/standardized_features.dat'\n",
        "standardized = np.memmap(output_path, dtype=np.float32,\n",
        "                        mode='w+', shape=features.shape)\n",
        "\n",
        "for i in range(0, len(features), chunk_size):\n",
        "    chunk = features[i:i+chunk_size]\n",
        "    standardized[i:i+chunk_size] = scaler.transform(chunk)\n",
        "\n",
        "# 5. Verify results\n",
        "print(\"Standardized data stats:\")\n",
        "print(f\"Mean: {np.mean(standardized):.4f}\")\n",
        "print(f\"Std: {np.std(standardized):.4f}\")\n",
        "\n",
        "# 6. Cleanup\n",
        "del features_memmap  # Release memory map\n",
        "standardized.flush()  # Ensure data is written to disk\n",
        "# Load the standardized features\n",
        "standardized = np.memmap(\n",
        "    '/home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/standardized_features.dat',\n",
        "    dtype=np.float32,\n",
        "    mode='r',  # Read-only mode\n",
        "    shape=features.shape  # Original feature dimensions\n",
        ")\n",
        "\n",
        "# Example: First 5 samples\n",
        "print(standardized[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "loading X and Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the saved data\n",
        "import numpy as np\n",
        "pd= r'/home/azureuser/cloudfiles/code/Users/pemphokatsala/DATA/processed_waste_dataset/processed_data.npz'\n",
        "data = np.load(pd)\n",
        "X = data['features']\n",
        "y = data['labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "training Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['LR_model.pkl']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "model = LogisticRegression(\n",
        "    max_iter=200,\n",
        "   solver='lbfgs',\n",
        "    #tol=1e-4,\n",
        "    #random_state=42,\n",
        ")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8)\n",
        "model.fit(X_train, y_train)\n",
        "model.predict(X_test)\n",
        "joblib.dump(model, 'LR_model.pkl')\n",
        "#model.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Training SCV Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1/197 | Test Accuracy: 0.7108\n",
            "Batch 2/197 | Test Accuracy: 0.6816\n",
            "Batch 3/197 | Test Accuracy: 0.7250\n",
            "Batch 4/197 | Test Accuracy: 0.7372\n",
            "Batch 5/197 | Test Accuracy: 0.6753\n",
            "Batch 6/197 | Test Accuracy: 0.7252\n",
            "Batch 7/197 | Test Accuracy: 0.6903\n",
            "Batch 8/197 | Test Accuracy: 0.7263\n",
            "Batch 9/197 | Test Accuracy: 0.6838\n",
            "Batch 10/197 | Test Accuracy: 0.7474\n",
            "Batch 11/197 | Test Accuracy: 0.6950\n",
            "Batch 12/197 | Test Accuracy: 0.7161\n",
            "Batch 13/197 | Test Accuracy: 0.6122\n",
            "Batch 14/197 | Test Accuracy: 0.7145\n",
            "Batch 15/197 | Test Accuracy: 0.6918\n",
            "Batch 16/197 | Test Accuracy: 0.6392\n",
            "Batch 17/197 | Test Accuracy: 0.7108\n",
            "Batch 18/197 | Test Accuracy: 0.6832\n",
            "Batch 19/197 | Test Accuracy: 0.7228\n",
            "Batch 20/197 | Test Accuracy: 0.7007\n",
            "Batch 21/197 | Test Accuracy: 0.6362\n",
            "Batch 22/197 | Test Accuracy: 0.7242\n",
            "Batch 23/197 | Test Accuracy: 0.7177\n",
            "Batch 24/197 | Test Accuracy: 0.5968\n",
            "Batch 25/197 | Test Accuracy: 0.6658\n",
            "Batch 26/197 | Test Accuracy: 0.7413\n",
            "Batch 27/197 | Test Accuracy: 0.6845\n",
            "Batch 28/197 | Test Accuracy: 0.6995\n",
            "Batch 29/197 | Test Accuracy: 0.7240\n",
            "Batch 30/197 | Test Accuracy: 0.7100\n",
            "Batch 31/197 | Test Accuracy: 0.7265\n",
            "Batch 32/197 | Test Accuracy: 0.7151\n",
            "Batch 33/197 | Test Accuracy: 0.7216\n",
            "Batch 34/197 | Test Accuracy: 0.6794\n",
            "Batch 35/197 | Test Accuracy: 0.7033\n",
            "Batch 36/197 | Test Accuracy: 0.7167\n",
            "Batch 37/197 | Test Accuracy: 0.7072\n",
            "Batch 38/197 | Test Accuracy: 0.7210\n",
            "Batch 39/197 | Test Accuracy: 0.6703\n",
            "Batch 40/197 | Test Accuracy: 0.7001\n",
            "Batch 41/197 | Test Accuracy: 0.6847\n",
            "Batch 42/197 | Test Accuracy: 0.7074\n",
            "Batch 43/197 | Test Accuracy: 0.7035\n",
            "Batch 44/197 | Test Accuracy: 0.7220\n",
            "Batch 45/197 | Test Accuracy: 0.7185\n",
            "Batch 46/197 | Test Accuracy: 0.6173\n",
            "Batch 47/197 | Test Accuracy: 0.7050\n",
            "Batch 48/197 | Test Accuracy: 0.7149\n",
            "Batch 49/197 | Test Accuracy: 0.7169\n",
            "Batch 50/197 | Test Accuracy: 0.7102\n",
            "Batch 51/197 | Test Accuracy: 0.6554\n",
            "Batch 52/197 | Test Accuracy: 0.7119\n",
            "Batch 53/197 | Test Accuracy: 0.7360\n",
            "Batch 54/197 | Test Accuracy: 0.7228\n",
            "Batch 55/197 | Test Accuracy: 0.7149\n",
            "Batch 56/197 | Test Accuracy: 0.6964\n",
            "Batch 57/197 | Test Accuracy: 0.7350\n",
            "Batch 58/197 | Test Accuracy: 0.6920\n",
            "Batch 59/197 | Test Accuracy: 0.7200\n",
            "Batch 60/197 | Test Accuracy: 0.7173\n",
            "Batch 61/197 | Test Accuracy: 0.7054\n",
            "Batch 62/197 | Test Accuracy: 0.6778\n",
            "Batch 63/197 | Test Accuracy: 0.7256\n",
            "Batch 64/197 | Test Accuracy: 0.6512\n",
            "Batch 65/197 | Test Accuracy: 0.7145\n",
            "Batch 66/197 | Test Accuracy: 0.7177\n",
            "Batch 67/197 | Test Accuracy: 0.6544\n",
            "Batch 68/197 | Test Accuracy: 0.6885\n",
            "Batch 69/197 | Test Accuracy: 0.6524\n",
            "Batch 70/197 | Test Accuracy: 0.5998\n",
            "Batch 71/197 | Test Accuracy: 0.7171\n",
            "Batch 72/197 | Test Accuracy: 0.7321\n",
            "Batch 73/197 | Test Accuracy: 0.6759\n",
            "Batch 74/197 | Test Accuracy: 0.6972\n",
            "Batch 75/197 | Test Accuracy: 0.7070\n",
            "Batch 76/197 | Test Accuracy: 0.7106\n",
            "Batch 77/197 | Test Accuracy: 0.6810\n",
            "Batch 78/197 | Test Accuracy: 0.6999\n",
            "Batch 79/197 | Test Accuracy: 0.7188\n",
            "Batch 80/197 | Test Accuracy: 0.7240\n",
            "Batch 81/197 | Test Accuracy: 0.6684\n",
            "Batch 82/197 | Test Accuracy: 0.7238\n",
            "Batch 83/197 | Test Accuracy: 0.6646\n",
            "Batch 84/197 | Test Accuracy: 0.6932\n",
            "Batch 85/197 | Test Accuracy: 0.7137\n",
            "Batch 86/197 | Test Accuracy: 0.7135\n",
            "Batch 87/197 | Test Accuracy: 0.7062\n",
            "Batch 88/197 | Test Accuracy: 0.7100\n",
            "Batch 89/197 | Test Accuracy: 0.6974\n",
            "Batch 90/197 | Test Accuracy: 0.6703\n",
            "Batch 91/197 | Test Accuracy: 0.7165\n",
            "Batch 92/197 | Test Accuracy: 0.7145\n",
            "Batch 93/197 | Test Accuracy: 0.6964\n",
            "Batch 94/197 | Test Accuracy: 0.7336\n",
            "Batch 95/197 | Test Accuracy: 0.7025\n",
            "Batch 96/197 | Test Accuracy: 0.7188\n",
            "Batch 97/197 | Test Accuracy: 0.6976\n",
            "Batch 98/197 | Test Accuracy: 0.6962\n",
            "Batch 99/197 | Test Accuracy: 0.7175\n",
            "Batch 100/197 | Test Accuracy: 0.7007\n",
            "Batch 101/197 | Test Accuracy: 0.7370\n",
            "Batch 102/197 | Test Accuracy: 0.7041\n",
            "Batch 103/197 | Test Accuracy: 0.6914\n",
            "Batch 104/197 | Test Accuracy: 0.7388\n",
            "Batch 105/197 | Test Accuracy: 0.7082\n",
            "Batch 106/197 | Test Accuracy: 0.7175\n",
            "Batch 107/197 | Test Accuracy: 0.7043\n",
            "Batch 108/197 | Test Accuracy: 0.7435\n",
            "Batch 109/197 | Test Accuracy: 0.7328\n",
            "Batch 110/197 | Test Accuracy: 0.6274\n",
            "Batch 111/197 | Test Accuracy: 0.6942\n",
            "Batch 112/197 | Test Accuracy: 0.7145\n",
            "Batch 113/197 | Test Accuracy: 0.7177\n",
            "Batch 114/197 | Test Accuracy: 0.7202\n",
            "Batch 115/197 | Test Accuracy: 0.7116\n",
            "Batch 116/197 | Test Accuracy: 0.6019\n",
            "Batch 117/197 | Test Accuracy: 0.6985\n",
            "Batch 118/197 | Test Accuracy: 0.7313\n",
            "Batch 119/197 | Test Accuracy: 0.7108\n",
            "Batch 120/197 | Test Accuracy: 0.6871\n",
            "Batch 121/197 | Test Accuracy: 0.7218\n",
            "Batch 122/197 | Test Accuracy: 0.6703\n",
            "Batch 123/197 | Test Accuracy: 0.6806\n",
            "Batch 124/197 | Test Accuracy: 0.7031\n",
            "Batch 125/197 | Test Accuracy: 0.7102\n",
            "Batch 126/197 | Test Accuracy: 0.7441\n",
            "Batch 127/197 | Test Accuracy: 0.7248\n",
            "Batch 128/197 | Test Accuracy: 0.7273\n",
            "Batch 129/197 | Test Accuracy: 0.7025\n",
            "Batch 130/197 | Test Accuracy: 0.6924\n",
            "Batch 131/197 | Test Accuracy: 0.7173\n",
            "Batch 132/197 | Test Accuracy: 0.7218\n",
            "Batch 133/197 | Test Accuracy: 0.7159\n",
            "Batch 134/197 | Test Accuracy: 0.6997\n",
            "Batch 135/197 | Test Accuracy: 0.7141\n",
            "Batch 136/197 | Test Accuracy: 0.7283\n",
            "Batch 137/197 | Test Accuracy: 0.7088\n",
            "Batch 138/197 | Test Accuracy: 0.6954\n",
            "Batch 139/197 | Test Accuracy: 0.7096\n",
            "Batch 140/197 | Test Accuracy: 0.6922\n",
            "Batch 141/197 | Test Accuracy: 0.7076\n",
            "Batch 142/197 | Test Accuracy: 0.6873\n",
            "Batch 143/197 | Test Accuracy: 0.6418\n",
            "Batch 144/197 | Test Accuracy: 0.7045\n",
            "Batch 145/197 | Test Accuracy: 0.7250\n",
            "Batch 146/197 | Test Accuracy: 0.7157\n",
            "Batch 147/197 | Test Accuracy: 0.7151\n",
            "Batch 148/197 | Test Accuracy: 0.6968\n",
            "Batch 149/197 | Test Accuracy: 0.7054\n",
            "Batch 150/197 | Test Accuracy: 0.7100\n",
            "Batch 151/197 | Test Accuracy: 0.7216\n",
            "Batch 152/197 | Test Accuracy: 0.7169\n",
            "Batch 153/197 | Test Accuracy: 0.7224\n",
            "Batch 154/197 | Test Accuracy: 0.6849\n",
            "Batch 155/197 | Test Accuracy: 0.7198\n",
            "Batch 156/197 | Test Accuracy: 0.7094\n",
            "Batch 157/197 | Test Accuracy: 0.7332\n",
            "Batch 158/197 | Test Accuracy: 0.7202\n",
            "Batch 159/197 | Test Accuracy: 0.7340\n",
            "Batch 160/197 | Test Accuracy: 0.6804\n",
            "Batch 161/197 | Test Accuracy: 0.7147\n",
            "Batch 162/197 | Test Accuracy: 0.7084\n",
            "Batch 163/197 | Test Accuracy: 0.6540\n",
            "Batch 164/197 | Test Accuracy: 0.6471\n",
            "Batch 165/197 | Test Accuracy: 0.7279\n",
            "Batch 166/197 | Test Accuracy: 0.7259\n",
            "Batch 167/197 | Test Accuracy: 0.7145\n",
            "Batch 168/197 | Test Accuracy: 0.7433\n",
            "Batch 169/197 | Test Accuracy: 0.6266\n",
            "Batch 170/197 | Test Accuracy: 0.7121\n",
            "Batch 171/197 | Test Accuracy: 0.6124\n",
            "Batch 172/197 | Test Accuracy: 0.6970\n",
            "Batch 173/197 | Test Accuracy: 0.5881\n",
            "Batch 174/197 | Test Accuracy: 0.7226\n",
            "Batch 175/197 | Test Accuracy: 0.6849\n",
            "Batch 176/197 | Test Accuracy: 0.6638\n",
            "Batch 177/197 | Test Accuracy: 0.7269\n",
            "Batch 178/197 | Test Accuracy: 0.7082\n",
            "Batch 179/197 | Test Accuracy: 0.5940\n",
            "Batch 180/197 | Test Accuracy: 0.6905\n",
            "Batch 181/197 | Test Accuracy: 0.7252\n",
            "Batch 182/197 | Test Accuracy: 0.7248\n",
            "Batch 183/197 | Test Accuracy: 0.7084\n",
            "Batch 184/197 | Test Accuracy: 0.7135\n",
            "Batch 185/197 | Test Accuracy: 0.7297\n",
            "Batch 186/197 | Test Accuracy: 0.7001\n",
            "Batch 187/197 | Test Accuracy: 0.6873\n",
            "Batch 188/197 | Test Accuracy: 0.7096\n",
            "Batch 189/197 | Test Accuracy: 0.7050\n",
            "Batch 190/197 | Test Accuracy: 0.6818\n",
            "Batch 191/197 | Test Accuracy: 0.7330\n",
            "Batch 192/197 | Test Accuracy: 0.6946\n",
            "Batch 193/197 | Test Accuracy: 0.7074\n",
            "Batch 194/197 | Test Accuracy: 0.7240\n",
            "Batch 195/197 | Test Accuracy: 0.7104\n",
            "Batch 196/197 | Test Accuracy: 0.7015\n",
            "Batch 197/197 | Test Accuracy: 0.7054\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['SVC_model.pkl']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "# 1. Split into train/test FIRST\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 2. Initialize scaler and fit on ENTIRE training data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)  # Critical: Use full training data for scaling\n",
        "\n",
        "# 3. Scale all data upfront\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Shuffle training data\n",
        "X_train_scaled, y_train = shuffle(X_train_scaled, y_train, random_state=42)\n",
        "\n",
        "# 5. Initialize model correctly\n",
        "model = SVC()\n",
        "\n",
        "# 6. Batch training parameters\n",
        "batch_size = 100  # Increased from 10\n",
        "num_batches = len(X_train_scaled) // batch_size\n",
        "\n",
        "# 7. Training loop with validation\n",
        "for batch_idx in range(num_batches):\n",
        "    # Get batch\n",
        "    start = batch_idx * batch_size\n",
        "    end = start + batch_size\n",
        "    X_batch = X_train_scaled[start:end]\n",
        "    y_batch = y_train[start:end]\n",
        "\n",
        "    # Train incrementally\n",
        "    model.fit(X_batch, y_batch)\n",
        "\n",
        "    # Validate on test set\n",
        "    test_acc = model.score(X_test_scaled, y_test)\n",
        "    print(f\"Batch {batch_idx+1}/{num_batches} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save model and scaler\n",
        "joblib.dump(model, 'SVC_model.pkl')\n",
        "#joblib.dump(scaler, 'scaler.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Checking the models' accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 0.9435876623376623\n",
            "Model accuracy: 0.5515422077922078\n"
          ]
        }
      ],
      "source": [
        "import joblib # This is to load your model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the saved model\n",
        "md = r'/home/azureuser/cloudfiles/code/LR_model.pkl'\n",
        "md2 = r'/home/azureuser/cloudfiles/code/SVC_model.pkl'\n",
        "model = joblib.load(md)\n",
        "model2 = joblib.load(md2)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "\n",
        "# Now you can predict and score without fitting again:\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = model.score(X_test, y_test)\n",
        "\n",
        "predictions2 = model2.predict(X_test)\n",
        "accuracy2 = model2.score(X_test, y_test)\n",
        "\n",
        "print(f\"Model accuracy: {accuracy}\")\n",
        "print(f\"Model accuracy: {accuracy2}\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - Pytorch and Tensorflow",
      "language": "python",
      "name": "python38-azureml-pt-tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
